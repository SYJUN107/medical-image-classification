{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X-Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Path to the dataset\n",
    "dataset_path = 'data/raw/covid-chest-xray'\n",
    "\n",
    "# Dictionary to store image counts per folder\n",
    "folder_image_counts = {}\n",
    "\n",
    "# Iterate through each subdirectory\n",
    "for folder in os.listdir(dataset_path):\n",
    "    folder_path = os.path.join(dataset_path, folder)\n",
    "    \n",
    "    # Check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # List all files in the folder\n",
    "        all_files = os.listdir(folder_path)\n",
    "        \n",
    "        # Filter image files (assuming they have common image extensions)\n",
    "        image_files = [f for f in all_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "        # Count the number of images in the folder\n",
    "        folder_image_counts[folder] = len(image_files)\n",
    "\n",
    "# Print the image counts per folder\n",
    "for folder, count in folder_image_counts.items():\n",
    "    print(f\"{folder}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignoramos bacterial o hacemos data-augmentation para balancear las clases?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos carpeta con las imagenes de train y test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the original dataset and output directory\n",
    "# dataset_path = \"data/raw/covid-chest-xray\"\n",
    "# output_path = \"data/split/covid-chest-xray\"\n",
    "\n",
    "# # Define train-test split ratio\n",
    "# train_ratio = 0.8\n",
    "\n",
    "# # Create train and test directories\n",
    "# train_path = os.path.join(output_path, \"train\")\n",
    "# test_path = os.path.join(output_path, \"test\")\n",
    "\n",
    "# # Ensure directories exist\n",
    "# os.makedirs(train_path, exist_ok=True)\n",
    "# os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "# # Iterate through each category (subfolder)\n",
    "# for category in os.listdir(dataset_path):\n",
    "#     category_path = os.path.join(dataset_path, category)\n",
    "    \n",
    "#     # Check if it's a directory\n",
    "#     if os.path.isdir(category_path):\n",
    "#         # Create corresponding train/test subfolders\n",
    "#         train_category_path = os.path.join(train_path, category)\n",
    "#         test_category_path = os.path.join(test_path, category)\n",
    "#         os.makedirs(train_category_path, exist_ok=True)\n",
    "#         os.makedirs(test_category_path, exist_ok=True)\n",
    "        \n",
    "#         # Get all image files\n",
    "#         all_images = [f for f in os.listdir(category_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "#         random.shuffle(all_images)  # Shuffle to randomize selection\n",
    "\n",
    "#         # Split data\n",
    "#         train_count = int(len(all_images) * train_ratio)\n",
    "#         train_images = all_images[:train_count]\n",
    "#         test_images = all_images[train_count:]\n",
    "\n",
    "#         # Move images to respective folders\n",
    "#         for img in train_images:\n",
    "#             shutil.copy2(os.path.join(category_path, img), os.path.join(train_category_path, img))\n",
    "        \n",
    "#         for img in test_images:\n",
    "#             shutil.copy2(os.path.join(category_path, img), os.path.join(test_category_path, img))\n",
    "\n",
    "#         print(f\"Processed {category}: {len(train_images)} train, {len(test_images)} test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/split/covid-chest-xray/train',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=25,\n",
    "    class_mode='binary') #class_mode='categorical' cambiar si metemos bacterial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'data/split/covid-chest-xray/test',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=25,\n",
    "    class_mode='binary') #class_mode='categorical' cambiar si metemos bacterial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model\n",
    "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "conv_base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_base.layers:\n",
    "    if layer.name.startswith('block5'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model and add the VGG16 base\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "\n",
    "# Add new layers on top of the VGG16 base\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50, #100\n",
    "    epochs=15, #30\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el modelo ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ResNet50 model\n",
    "conv_base = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "conv_base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in conv_base.layers:\n",
    "    if layer.name.startswith('conv5'):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model and add the ResNet50 base\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "\n",
    "# Add new layers on top of the ResNet50 base\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=['accuracy'])\n",
    "#model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(learning_rate=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50, #100\n",
    "    epochs=15, #30\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((150, 150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/split/covid-chest-xray'\n",
    "train_dataset = datasets.ImageFolder(root=f'{data_dir}/train', transform=data_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=f'{data_dir}/test', transform=data_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 1)\n",
    "model = model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "perdida = []\n",
    "accuracy = []\n",
    "\n",
    "for epoch in range(15):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).float().view(-1, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.round(torch.sigmoid(outputs))\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    perdida.append(epoch_loss)\n",
    "    accuracy.append(epoch_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/15 - Loss: {epoch_loss:.4f} - Accuracy: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(accuracy)\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,2)\n",
    "plt.plot(perdida)\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
